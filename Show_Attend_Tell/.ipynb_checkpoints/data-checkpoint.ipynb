{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.optim import lr_scheduler\n",
    "# import numpy as np\n",
    "# import time\n",
    "# import os\n",
    "# import argparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the model \n",
    "model = torchvision.models.vgg19_bn(pretrained='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/pseulki/문서/GoogLeNet /Deep-Learning-Paper/Show_Attend_Tell'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace)\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ReLU(inplace)\n",
       "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): ReLU(inplace)\n",
       "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): ReLU(inplace)\n",
       "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (19): ReLU(inplace)\n",
       "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (22): ReLU(inplace)\n",
       "    (23): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (25): ReLU(inplace)\n",
       "    (26): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (27): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (29): ReLU(inplace)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (32): ReLU(inplace)\n",
       "    (33): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (34): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (35): ReLU(inplace)\n",
       "    (36): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (37): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (38): ReLU(inplace)\n",
       "    (39): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (42): ReLU(inplace)\n",
       "    (43): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (44): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (45): ReLU(inplace)\n",
       "    (46): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (47): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (48): ReLU(inplace)\n",
       "    (49): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (50): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (51): ReLU(inplace)\n",
       "    (52): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Dropout(p=0.5)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace)\n",
       "    (5): Dropout(p=0.5)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = model._modules.get('7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'VGG' object does not support indexing",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-3ee95d0696d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m27\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'VGG' object does not support indexing"
     ]
    }
   ],
   "source": [
    "model[27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding:utf8\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import vgg19_bn\n",
    "from collections import namedtuple\n",
    "\n",
    "class Vgg19(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Vgg19, self).__init__()\n",
    "        features = list(vgg19_bn(pretrained = True).features)[:23]\n",
    "        # features的第3，8，15，22层分别是: relu1_2,relu2_2,relu3_3,relu4_3\n",
    "        self.features = nn.ModuleList(features).eval() \n",
    "        \n",
    "    def forward(self, x):\n",
    "        results = []\n",
    "        for ii,model in enumerate(self.features):\n",
    "            x = model(x)\n",
    "            print(\"hey\", ii, x)\n",
    "            \n",
    "            if ii in {3,8,15,22}:\n",
    "                results.append(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.56s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.27s)\n",
      "creating index...\n",
      "index created!\n",
      "Vgg19(\n",
      "  (features): ModuleList(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace)\n",
      "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace)\n",
      "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace)\n",
      "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace)\n",
      "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): ReLU(inplace)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 4-dimensional input for 4-dimensional weight [64, 3, 3, 3], but got input of size [3, 224, 224] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-7a779adcdcbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;31m#def main():\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0mcoco_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m \u001b[0mcoco_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-7a779adcdcbc>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0;31m#outputs = self.decoder(features, captions, lengths)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/myenv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-b73c61681350>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mii\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mii\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m22\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/myenv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/myenv/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 301\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight [64, 3, 3, 3], but got input of size [3, 224, 224] instead"
     ]
    }
   ],
   "source": [
    "import torchvision.datasets as dset\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "class Encoder(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        scale_transform = transforms.Compose([\n",
    "            #transforms.ToPILImage(),\n",
    "            transforms.Resize(224),\n",
    "            transforms.RandomCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "        self.data_loader = {}\n",
    "        for data_type in ['train', 'val']:\n",
    "            coco_dataset = dset.CocoCaptions(root='./cocoData/2014_images/{}2014'.format(data_type),\n",
    "                                             annFile='./cocoData/annotations/captions_{}2014.json'.format(data_type),\n",
    "                                             transform=scale_transform)\n",
    "\n",
    "            self.data_loader[data_type] = torch.utils.data.DataLoader(dataset=coco_dataset,batch_size=128, shuffle=True, \n",
    "                                                                      num_workers=4, collate_fn=self.collate_fn)\n",
    "\n",
    "        embed_size = 2048\n",
    "        hidden_size = 1024\n",
    "        num_layers = 3\n",
    "        self.encoder = Vgg19().to(self.device)\n",
    "        print(Vgg19())\n",
    "        #self.decoder = DecoderRNN(embed_size, hidden_size, len(self.voca), num_layers).to(self.device)\n",
    "\n",
    "        #self.criterion = nn.CrossEntropyLoss()\n",
    "        #params = list(self.decoder.parameters()) + list(self.encoder.linear.parameters()) + list(self.encoder.bn.parameters())\n",
    "        #self.optimizer = torch.optim.Adam(params, lr=1e-3)\n",
    "\n",
    "    def collate_fn(self, data):\n",
    "        #max_len = self.max_cap_len\n",
    "\n",
    "        data.sort(key=lambda x: len(x[1][0]), reverse=True)\n",
    "        images, captions = zip(*data)\n",
    "\n",
    "# \t\tdef caption_to_idx_vector(caption):\n",
    "# \t\t\twords = CocoVoca.split_caption(caption)\n",
    "# \t\t\tidx_vector = [self.voca.get_idx_from_word('<start>')]\n",
    "# \t\t\tidx_vector += [self.voca.get_idx_from_word(word) for word in words]\n",
    "# \t\t\tidx_vector += [self.voca.get_idx_from_word('<end>')]\n",
    "\n",
    "# \t\t\tpad_count = max_len - len(idx_vector)\n",
    "# \t\t\tif pad_count > 0:\n",
    "# \t\t\t\tidx_vector += [self.voca.get_idx_from_word('<padding>')] * pad_count\n",
    "# \t\t\tif len(idx_vector) > max_len:\n",
    "# \t\t\t\tidx_vector = idx_vector[:max_len]\n",
    "\n",
    "# \t\t\treturn idx_vector\n",
    "\n",
    "        # Merge images (from tuple of 3D tensor to 4D tensor)\n",
    "        images = torch.stack(images, 0)\n",
    "\n",
    "        # TODO - only using first caption\n",
    "#         lengths = [min(len(cap[0]), max_len) for cap in captions]\n",
    "\n",
    "#         targets = torch.LongTensor(\n",
    "#             [caption_to_idx_vector(cap[0]) for cap in captions]\n",
    "#         )\n",
    "    \n",
    "\n",
    "        return images\n",
    "\n",
    "\n",
    "    \n",
    "    def train(self):\n",
    "        for i, data in enumerate(self.data_loader['train'], 0):\n",
    "            images, captions, lengths = data[0].to(self.device), data[1].to(self.device), data[2]\n",
    "\n",
    "            #targets = pack_padded_sequence(captions, lengths, batch_first=True)[0]\n",
    "\n",
    "\n",
    "            features = self.encoder(images)\n",
    "            #outputs = self.decoder(features, captions, lengths)\n",
    "\n",
    "            #loss = self.criterion(outputs, targets)\n",
    "\n",
    "            #self.decoder.zero_grad()\n",
    "            #self.encoder.zero_grad()\n",
    "\n",
    "            #loss.backward()\n",
    "            #self.optimizer.step()\n",
    "\n",
    "\n",
    "    #def main():\n",
    "coco_data = Encoder()\n",
    "coco_data.train()\n",
    "\n",
    "\n",
    "#if __name__ == '__main__':\n",
    " #   main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pseulki/anaconda3/envs/myenv/lib/python3.6/site-packages/torchvision/transforms/transforms.py:563: UserWarning: The use of the transforms.RandomSizedCrop transform is deprecated, please use transforms.RandomResizedCrop instead.\n",
      "  \"please use transforms.RandomResizedCrop instead.\")\n",
      "/home/pseulki/anaconda3/envs/myenv/lib/python3.6/site-packages/torchvision/transforms/transforms.py:188: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  \"please use transforms.Resize instead.\")\n"
     ]
    }
   ],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomSizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Scale(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-636abe9477af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./cocoDdata/2014_images/valid2014'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m dsets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])\n\u001b[0;32m----> 3\u001b[0;31m          for x in ['train', 'val']}\n\u001b[0m\u001b[1;32m      4\u001b[0m dset_loaders = {x: torch.utils.data.DataLoader(dsets[x], batch_size=BATCH_SIZE,\n\u001b[1;32m      5\u001b[0m                                                shuffle=True, num_workers=25)\n",
      "\u001b[0;32m<ipython-input-26-636abe9477af>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./cocoDdata/2014_images/valid2014'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m dsets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])\n\u001b[0;32m----> 3\u001b[0;31m          for x in ['train', 'val']}\n\u001b[0m\u001b[1;32m      4\u001b[0m dset_loaders = {x: torch.utils.data.DataLoader(dsets[x], batch_size=BATCH_SIZE,\n\u001b[1;32m      5\u001b[0m                                                shuffle=True, num_workers=25)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'datasets' is not defined"
     ]
    }
   ],
   "source": [
    "data_dir = './cocoDdata/2014_images/valid2014'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vgg19Bottom(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(Vgg19Bottom, self).__init__()\n",
    "        self.features = nn.Sequential(*list(original_model.children())[:-2])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return x\n",
    "\n",
    "vgg19_model = torchvision.models.vgg19_bn(pretrained='True')\n",
    "vgg19_layer = Vgg19Bottom(vgg19_model)\n",
    "\n",
    "inputs = \n",
    "\n",
    "outputs = vgg19_layer(inputs)\n",
    "outputs.data.shape  # => torch.Size([4, 2048, 7, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pic_one = str(input(\"Input first image name\\n\"))\n",
    "pic_two = str(input(\"Input second image name\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained model\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Use the model object to select the desired layer\n",
    "layer = model._modules.get('avgpool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer4 = model._modules.get('layer4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pseulki/anaconda3/envs/myenv/lib/python3.6/site-packages/torchvision/transforms/transforms.py:188: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  \"please use transforms.Resize instead.\")\n"
     ]
    }
   ],
   "source": [
    "scaler = transforms.Scale((224, 224))\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "to_tensor = transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(image_name):\n",
    "    # 1. Load the image with Pillow library\n",
    "    img = Image.open(image_name)\n",
    "    print(\"img\",img.size)\n",
    "\n",
    "    # 2. Create a PyTorch Variable with the transformed image\n",
    "    t_img = Variable(normalize(to_tensor(scaler(img))).unsqueeze(0))\n",
    "    print(\"t_img\", t_img.shape)\n",
    "\n",
    "    # 3. Create a vector of zeros that will hold our feature vector\n",
    "    #    The 'avgpool' layer has an output size of 512\n",
    "    my_embedding = torch.zeros((512))\n",
    "    print(\"my_embedding\", my_embedding.shape)\n",
    "\n",
    "    # 4. Define a function that will copy the output of a layer\n",
    "    def copy_data(m, i, o):\n",
    "        o = o.squeeze()\n",
    "        my_embedding.copy_(o.data)\n",
    "\n",
    "    # 5. Attach that function to our selected layer\n",
    "    h = layer.register_forward_hook(copy_data)\n",
    "\n",
    "    # 6. Run the model on our transformed image\n",
    "    model(t_img)\n",
    "\n",
    "    # 7. Detach our copy function from the layer\n",
    "    h.remove()\n",
    "\n",
    "    # 8. Return the feature vector\n",
    "    return my_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pic_one = './cocoData/2014_images/val2014/COCO_val2014_000000000042.jpg'\n",
    "#pic_two = str(input(\"Input second image name\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img (640, 478)\n",
      "t_img torch.Size([1, 3, 224, 224])\n",
      "my_embedding torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "pic_one_vector = get_vector(pic_one)\n",
    "#pic_two_vector = get_vector(pic_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4399, 0.6342, 1.3584, 0.5091, 1.2606, 0.5232, 1.6452, 0.4958, 0.3792,\n",
       "        0.3139, 0.5449, 0.4658, 2.1287, 0.5199, 0.1736, 0.2278, 1.0287, 0.7484,\n",
       "        0.4829, 2.9537, 1.2364, 1.3784, 1.5034, 2.4547, 1.6772, 0.1800, 2.6263,\n",
       "        0.4329, 0.6041, 1.7766, 0.8225, 1.2255, 0.4866, 0.3555, 2.9793, 0.2356,\n",
       "        2.5173, 1.3117, 0.4848, 0.5054, 0.9413, 0.7129, 2.0506, 0.3281, 0.8342,\n",
       "        1.1979, 2.5832, 1.8542, 0.3723, 2.0673, 0.7685, 1.4601, 1.5623, 0.3346,\n",
       "        0.3897, 1.1458, 2.3910, 2.3489, 0.2356, 2.3868, 1.0427, 1.5089, 1.6570,\n",
       "        1.1291, 0.8769, 0.5381, 0.7901, 0.0892, 0.7305, 0.6637, 0.5341, 0.0264,\n",
       "        0.0000, 0.6801, 0.1798, 1.8910, 0.1942, 0.2819, 1.4086, 2.4217, 3.2442,\n",
       "        2.4124, 1.0389, 2.4833, 0.7858, 3.2452, 0.0485, 1.7490, 0.1507, 0.9232,\n",
       "        1.2418, 0.0444, 3.0378, 1.1681, 0.0699, 0.8672, 0.7758, 2.1272, 0.2298,\n",
       "        0.5837, 5.4451, 0.5404, 2.3048, 1.7140, 0.2307, 0.5631, 1.0862, 3.2045,\n",
       "        0.0101, 1.2669, 0.2928, 0.7542, 2.5226, 0.8232, 0.4882, 1.2032, 1.7020,\n",
       "        0.3551, 1.5177, 0.3746, 0.2379, 1.2089, 1.9264, 0.7881, 1.6893, 0.3685,\n",
       "        1.3668, 1.6252, 0.9161, 2.6750, 2.0530, 0.9895, 1.0189, 0.8272, 0.5317,\n",
       "        3.4438, 0.1048, 0.0545, 0.8078, 0.5538, 2.3492, 0.1756, 0.1949, 0.7656,\n",
       "        0.4971, 0.7671, 3.5687, 0.2590, 0.3969, 0.0814, 0.0304, 0.8320, 0.7521,\n",
       "        0.1903, 2.0302, 1.0199, 1.7229, 1.4168, 0.9512, 2.9637, 0.3719, 0.2578,\n",
       "        2.3721, 2.0021, 1.2707, 1.2132, 1.0820, 0.7985, 1.7987, 0.9538, 0.9261,\n",
       "        0.0579, 0.6328, 0.2499, 0.3809, 0.3007, 1.7324, 0.9498, 0.1742, 0.4571,\n",
       "        1.9746, 1.0486, 0.9079, 0.2304, 2.2262, 2.5176, 1.1377, 0.2191, 0.5170,\n",
       "        1.3690, 2.0203, 0.6079, 0.3727, 0.4585, 2.8893, 1.4477, 0.5756, 1.2643,\n",
       "        0.3423, 0.4221, 0.8392, 0.4727, 0.4546, 1.1056, 1.3864, 1.6628, 0.8948,\n",
       "        1.6361, 0.1585, 2.4193, 0.9593, 0.5468, 0.0158, 1.0262, 0.8715, 0.2343,\n",
       "        1.6118, 0.2323, 0.4369, 1.0926, 2.2025, 1.0856, 3.9170, 1.9738, 0.5834,\n",
       "        0.3242, 0.3442, 0.9157, 0.8304, 0.6785, 1.5348, 0.5732, 0.6682, 1.2905,\n",
       "        0.9499, 0.4661, 0.4157, 0.6496, 2.1319, 0.6972, 0.7769, 1.8284, 0.2351,\n",
       "        1.1164, 0.1374, 2.1654, 1.3337, 1.8948, 0.4376, 0.6804, 0.2404, 0.4552,\n",
       "        0.7550, 0.6841, 1.5110, 0.0882, 0.4750, 0.2381, 2.3137, 0.7499, 0.5845,\n",
       "        0.4680, 0.2468, 0.1684, 1.0251, 1.4491, 0.8935, 0.8846, 0.6111, 0.0348,\n",
       "        0.2105, 0.3947, 1.2749, 0.0674, 0.2021, 1.6622, 0.4658, 1.5010, 0.8068,\n",
       "        0.8120, 0.8430, 2.9743, 0.3880, 0.4194, 0.7553, 0.8657, 0.1518, 0.1402,\n",
       "        0.0800, 1.0042, 2.5862, 0.2818, 1.1718, 0.4638, 0.6672, 0.1647, 1.4104,\n",
       "        0.2296, 0.0604, 0.0188, 0.9450, 2.8067, 0.1109, 2.8193, 0.8677, 1.6881,\n",
       "        0.2881, 1.1439, 0.1749, 1.1458, 1.4406, 0.2875, 1.0006, 1.2912, 0.3313,\n",
       "        0.8333, 2.1195, 0.6034, 0.6045, 0.9518, 0.0758, 1.0140, 0.6552, 3.3829,\n",
       "        1.2196, 0.2042, 2.3703, 1.7368, 0.0517, 0.7391, 1.9781, 1.0037, 1.5649,\n",
       "        0.7004, 0.3315, 1.6928, 0.0286, 0.1789, 1.6131, 0.0633, 0.8577, 0.6450,\n",
       "        0.3644, 0.7440, 0.0175, 0.2551, 2.1045, 0.1393, 0.0423, 0.7965, 0.7251,\n",
       "        0.0352, 0.8358, 2.1545, 0.2642, 0.0447, 1.7842, 1.8743, 0.4509, 1.0191,\n",
       "        2.0548, 1.7052, 0.0335, 1.4462, 1.4450, 0.6688, 0.3118, 4.4842, 0.4289,\n",
       "        1.6666, 1.6112, 0.0060, 2.5857, 1.0909, 1.3980, 0.3044, 0.7880, 1.5048,\n",
       "        4.0206, 2.3508, 0.9237, 0.7020, 1.9190, 0.5622, 1.5131, 0.6707, 2.2316,\n",
       "        3.8855, 0.4834, 1.9667, 0.1746, 0.5977, 0.7541, 0.9788, 1.0403, 1.3382,\n",
       "        0.1667, 1.1155, 0.6196, 1.7247, 0.2744, 0.6201, 0.5492, 1.9527, 1.1702,\n",
       "        1.8501, 2.7689, 1.1071, 0.7251, 0.1219, 2.4497, 1.1108, 1.1533, 0.4984,\n",
       "        1.2121, 0.9121, 0.4960, 1.7158, 0.3614, 1.1660, 0.2191, 2.1447, 0.4984,\n",
       "        0.3954, 0.1373, 1.8933, 1.1106, 0.6927, 0.2562, 0.0128, 2.2004, 1.6813,\n",
       "        0.2353, 0.1595, 0.6977, 0.4990, 1.6077, 2.0802, 1.2904, 2.5793, 0.4453,\n",
       "        0.2024, 0.5935, 0.7076, 0.5129, 1.2285, 0.2019, 0.4939, 1.5542, 1.0457,\n",
       "        0.1803, 2.6303, 0.4171, 0.6863, 1.1073, 0.0014, 1.2196, 0.9283, 1.1646,\n",
       "        0.0038, 1.9692, 2.5374, 1.6819, 1.1904, 0.6417, 0.3898, 0.3861, 0.6691,\n",
       "        0.2522, 0.1522, 1.2493, 0.1213, 0.6670, 1.2748, 1.9583, 0.9283, 0.6165,\n",
       "        0.3125, 1.4872, 0.3673, 0.0823, 1.4863, 2.1648, 0.1839, 3.2797, 0.5390,\n",
       "        1.0917, 1.3176, 0.6749, 0.6076, 0.7938, 0.1304, 1.5298, 1.7370, 1.8081,\n",
       "        0.7392, 0.0669, 1.3141, 1.6245, 0.9914, 2.6181, 1.2690, 1.4183, 0.2245,\n",
       "        0.6930, 0.9454, 0.6372, 0.5684, 0.3470, 0.2344, 0.8949, 0.5832])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pic_one_vector"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
