{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "reference: https://github.com/alwynmathew/Vanilla-GAN/blob/master/GAN_1.ipynb\n",
    "Changed convolution layers to FC layers as GAN. \n",
    "\"\"\"\n",
    "\n",
    "import os #,argparse \n",
    "import gzip\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch, time, pickle\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"input arguments\"\"\"\n",
    "\n",
    "dataset = 'mnist'\n",
    "epoch = 25 #25\n",
    "batch_size = 128\n",
    "sample_num = 100 #16\n",
    "save_dir = './models/FC'\n",
    "result_dir = './results/FC'\n",
    "log_dir = './logs/FC'\n",
    "lrG = 0.0002\n",
    "lrD = 0.0002\n",
    "beta1 = 0.5\n",
    "beta2 = 0.999\n",
    "gpu_mode = False\n",
    "model_name = 'GAN_FC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"checking arguments\"\"\"\n",
    "\n",
    "# --save_dir\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# --result_dir\n",
    "if not os.path.exists(result_dir):\n",
    "    os.makedirs(result_dir)\n",
    "\n",
    "# --result_dir\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"print network\"\"\"\n",
    "\n",
    "def print_network(net):\n",
    "    num_params = 0\n",
    "    for param in net.parameters():\n",
    "        num_params += param.numel()\n",
    "    print(net)\n",
    "    print('Total number of parameters: %d' % num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"save images\"\"\"\n",
    "\n",
    "def save_images(images, size, image_path):\n",
    "    return imsave(images, size, image_path)\n",
    "\n",
    "def imsave(images, size, path):\n",
    "    image = np.squeeze(merge(images, size))\n",
    "    return scipy.misc.imsave(path, image)\n",
    "\n",
    "\"\"\"merge images\"\"\"\n",
    "\n",
    "def merge(images, size):\n",
    "    h, w = images.shape[1], images.shape[2]\n",
    "    if (images.shape[3] in (3,4)):\n",
    "        c = images.shape[3]\n",
    "        img = np.zeros((h * size[0], w * size[1], c))\n",
    "        for idx, image in enumerate(images):\n",
    "            i = idx % size[1]\n",
    "            j = idx // size[1]\n",
    "            img[j * h:j * h + h, i * w:i * w + w, :] = image\n",
    "        return img\n",
    "    elif images.shape[3]==1:\n",
    "        img = np.zeros((h * size[0], w * size[1]))\n",
    "        for idx, image in enumerate(images):\n",
    "            i = idx % size[1]\n",
    "            j = idx // size[1]\n",
    "            img[j * h:j * h + h, i * w:i * w + w] = image[:,:,0]\n",
    "        return img\n",
    "    else:\n",
    "        raise ValueError('in merge(images,size) images parameter ''must have dimensions: HxW or HxWx3 or HxWx4')\n",
    "\n",
    "\"\"\"generate animation\"\"\"\n",
    "        \n",
    "def generate_animation(path, num):\n",
    "    images = []\n",
    "    for e in range(num):\n",
    "        img_name = path + '_epoch%03d' % (e+1) + '.png'\n",
    "        images.append(imageio.imread(img_name))\n",
    "    imageio.mimsave(path + '_generate_animation.gif', images, fps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"plot loss\"\"\"\n",
    "\n",
    "def loss_plot(hist, path = 'Train_hist.png', model_name = ''):\n",
    "    x = range(len(hist['D_loss']))\n",
    "\n",
    "    y1 = hist['D_loss']\n",
    "    y2 = hist['G_loss']\n",
    "\n",
    "    plt.plot(x, y1, label='D_loss')\n",
    "    plt.plot(x, y2, label='G_loss')\n",
    "\n",
    "    plt.xlabel('Iter')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.legend(loc=4)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    path = os.path.join(path, model_name + '_loss.png')\n",
    "\n",
    "    plt.savefig(path)\n",
    "\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"initialize weights\"\"\"\n",
    "\n",
    "def initialize_weights(net):\n",
    "    for m in net.modules():\n",
    "\n",
    "        if isinstance(m, nn.Linear):\n",
    "            m.weight.data.normal_(0, 0.02)\n",
    "            m.bias.data.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"generator\"\"\"\n",
    "\n",
    "class generator(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        #print('---------- generator -------------')\n",
    "        super(generator, self).__init__()\n",
    "\n",
    "        self.input_height = 28\n",
    "        self.input_width = 28\n",
    "        self.input_dim = 62\n",
    "        self.output_dim = 1\n",
    "\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 2048),\n",
    "            nn.BatchNorm1d(2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 784),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        #utils.\n",
    "        initialize_weights(self)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        x = self.fc(input)\n",
    "        x = x.view(-1, 1, 28, 28)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"discriminator\"\"\"\n",
    "\n",
    "class discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(discriminator, self).__init__()\n",
    "\n",
    "        self.input_height = 28\n",
    "        self.input_width = 28\n",
    "        self.input_dim = 1\n",
    "        self.output_dim = 1\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(784, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(1024, 2048),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(2048,1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        #utils.\n",
    "        initialize_weights(self)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        x = input.view(-1, 784)\n",
    "        #print(\"x11\",x.shape)\n",
    "        x = self.fc(x)\n",
    "        #print(\"x22\", x.shape)\n",
    "        x = self.fc2(x)\n",
    "        #print(\"x333\", x.shape)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN():\n",
    "    def __init__(self):\n",
    "\n",
    "        # networks init\n",
    "        self.G = generator()\n",
    "        self.D = discriminator()\n",
    "        #print('---------- GAN -------------')\n",
    "        self.G_optimizer = optim.Adam(self.G.parameters(), lr=lrG, betas=(beta1, beta2))\n",
    "        self.D_optimizer = optim.Adam(self.D.parameters(), lr=lrD, betas=(beta1, beta2))\n",
    "\n",
    "\n",
    "        self.G.to(device)\n",
    "        self.D.to(device)\n",
    "        self.BCE_loss = nn.BCELoss().to(device)\n",
    "\n",
    "\n",
    "        print('---------- Networks architecture -------------')\n",
    "        #utils.\n",
    "        print_network(self.G)\n",
    "        print('-----------------------------------------------')\n",
    "        #utils.\n",
    "        print_network(self.D)\n",
    "        print('-----------------------------------------------')\n",
    "\n",
    "        # load dataset\n",
    "\n",
    "        self.data_loader = DataLoader(datasets.MNIST('./data', train=True, download=True,\n",
    "                                                                      transform=transforms.Compose(\n",
    "                                                                          [transforms.ToTensor()])),\n",
    "                                                       batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        \n",
    "        print (\"Size of %s data loader : \" % (dataset), self.data_loader.dataset.__len__())\n",
    "        print('-----------------------------------------------')\n",
    "        self.z_dim = 62\n",
    "\n",
    "        # fixed noise\n",
    "        self.sample_z_ = Variable(torch.rand((batch_size, self.z_dim))).to(device)\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        self.train_hist = {}\n",
    "        self.train_hist['D_loss'] = []\n",
    "        self.train_hist['G_loss'] = []\n",
    "        self.train_hist['per_epoch_time'] = []\n",
    "        self.train_hist['total_time'] = []\n",
    "\n",
    "\n",
    "        self.y_real_, self.y_fake_ = Variable(torch.ones(batch_size, 1)).to(device), Variable(torch.zeros(batch_size, 1)).to(device)\n",
    "\n",
    "\n",
    "        self.D.train()\n",
    "        print('training start!!')\n",
    "        start_time = time.time()\n",
    "        for epochs in range(epoch):\n",
    "            self.G.train()\n",
    "            epoch_start_time = time.time()\n",
    "            for iter, (x_, _) in enumerate(self.data_loader):\n",
    "                if iter == self.data_loader.dataset.__len__() // batch_size:\n",
    "                    break\n",
    "\n",
    "                z_ = torch.rand((batch_size, self.z_dim))\n",
    "\n",
    "                x_, z_ = Variable(x_).to(device), Variable(z_).to(device)\n",
    "\n",
    "\n",
    "                # update D network\n",
    "                self.D_optimizer.zero_grad()\n",
    "\n",
    "                D_real = self.D(x_)\n",
    "                D_real_loss = self.BCE_loss(D_real, self.y_real_)\n",
    "\n",
    "                G_ = self.G(z_)\n",
    "                D_fake = self.D(G_)\n",
    "                D_fake_loss = self.BCE_loss(D_fake, self.y_fake_)\n",
    "\n",
    "                D_loss = D_real_loss + D_fake_loss\n",
    "                self.train_hist['D_loss'].append(D_loss.data[0])\n",
    "\n",
    "                D_loss.backward()\n",
    "                self.D_optimizer.step()\n",
    "\n",
    "                # update G network\n",
    "                self.G_optimizer.zero_grad()\n",
    "\n",
    "                G_ = self.G(z_)\n",
    "                D_fake = self.D(G_)\n",
    "                G_loss = self.BCE_loss(D_fake, self.y_real_)\n",
    "                self.train_hist['G_loss'].append(G_loss.data[0])\n",
    "\n",
    "                G_loss.backward()\n",
    "                self.G_optimizer.step()\n",
    "\n",
    "                if ((iter + 1) % 100) == 0:\n",
    "                    print(\"Epoch: [%2d] [%4d/%4d] D_loss: %.8f, G_loss: %.8f\" %\n",
    "                          ((epochs + 1), (iter + 1), self.data_loader.dataset.__len__() // batch_size, D_loss.data[0], G_loss.data[0]))\n",
    "\n",
    "            self.train_hist['per_epoch_time'].append(time.time() - epoch_start_time)\n",
    "            self.visualize_results((epochs+1))\n",
    "\n",
    "        self.train_hist['total_time'].append(time.time() - start_time)\n",
    "        print(\"Avg one epoch time: %.2f, total %d epochs time: %.2f\" % (np.mean(self.train_hist['per_epoch_time']),\n",
    "              epoch, self.train_hist['total_time'][0]))\n",
    "        print(\"Training finish!... save training results\")\n",
    "\n",
    "        self.save()\n",
    "        #utils.\n",
    "        generate_animation(result_dir + '/' + dataset + '/' + model_name + '/' + model_name,\n",
    "                                 epoch)\n",
    "        #utils.\n",
    "        loss_plot(self.train_hist, os.path.join(save_dir, dataset, model_name), model_name)\n",
    "\n",
    "    def visualize_results(self, epochs, fix=True):\n",
    "        self.G.eval()\n",
    "\n",
    "        if not os.path.exists(result_dir + '/' + dataset + '/' + model_name):\n",
    "            os.makedirs(result_dir + '/' + dataset + '/' + model_name)\n",
    "\n",
    "        tot_num_samples = min(sample_num, batch_size)\n",
    "        image_frame_dim = int(np.floor(np.sqrt(tot_num_samples)))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            if fix:\n",
    "                \"\"\" fixed noise \"\"\"\n",
    "                samples = self.G(self.sample_z_)\n",
    "            else:\n",
    "                \"\"\" random noise \"\"\"\n",
    "\n",
    "                sample_z_ = Variable(torch.rand((batch_size, self.z_dim))).to(device)\n",
    "\n",
    "                samples = self.G(sample_z_)\n",
    "\n",
    "\n",
    "            samples = samples.cpu().data.numpy().transpose(0, 2, 3, 1)\n",
    "\n",
    "\n",
    "        #utils.\n",
    "        save_images(samples[:image_frame_dim * image_frame_dim, :, :, :], [image_frame_dim, image_frame_dim],\n",
    "                          result_dir + '/' + dataset + '/' + model_name + '/' + model_name + '_epoch%03d' % epochs + '.png')\n",
    "    \n",
    "    def forward(self, input):\n",
    "        x = self.conv(input)\n",
    "        x = x.view(-1, 128 * (self.input_height // 4) * (self.input_width // 4))\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "    def save(self):\n",
    "        save_dir_full = os.path.join(save_dir, dataset, model_name)\n",
    "\n",
    "        if not os.path.exists(save_dir_full):\n",
    "            os.makedirs(save_dir_full)\n",
    "\n",
    "        torch.save(self.G.state_dict(), os.path.join(save_dir_full, model_name + '_G.pkl'))\n",
    "        torch.save(self.D.state_dict(), os.path.join(save_dir_full, model_name + '_D.pkl'))\n",
    "\n",
    "        with open(os.path.join(save_dir_full, model_name + '_history.pkl'), 'wb') as f:\n",
    "            pickle.dump(self.train_hist, f)\n",
    "\n",
    "    def load(self):\n",
    "        save_dir_full = os.path.join(save_dir, dataset, model_name)\n",
    "\n",
    "        self.G.load_state_dict(torch.load(os.path.join(save_dir_full, model_name + '_G.pkl')))\n",
    "        self.D.load_state_dict(torch.load(os.path.join(save_dir_full, model_name + '_D.pkl')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Networks architecture -------------\n",
      "generator(\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=62, out_features=1024, bias=True)\n",
      "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "    (4): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=2048, out_features=784, bias=True)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Total number of parameters: 3776272\n",
      "-----------------------------------------------\n",
      "discriminator(\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1024, bias=True)\n",
      "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (fc2): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.2)\n",
      "    (2): Linear(in_features=2048, out_features=1, bias=True)\n",
      "    (3): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Total number of parameters: 2907137\n",
      "-----------------------------------------------\n",
      "Size of mnist data loader :  60000\n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\"\"\"run GAN\"\"\"\n",
    "gan = GAN()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training start!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pseulki/anaconda3/envs/myenv/lib/python3.6/site-packages/ipykernel_launcher.py:88: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/home/pseulki/anaconda3/envs/myenv/lib/python3.6/site-packages/ipykernel_launcher.py:99: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/home/pseulki/anaconda3/envs/myenv/lib/python3.6/site-packages/ipykernel_launcher.py:106: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 1] [ 100/ 468] D_loss: 1.00089478, G_loss: 1.21788812\n",
      "Epoch: [ 1] [ 200/ 468] D_loss: 0.95763034, G_loss: 1.34242034\n",
      "Epoch: [ 1] [ 300/ 468] D_loss: 1.11121500, G_loss: 1.31351554\n",
      "Epoch: [ 1] [ 400/ 468] D_loss: 0.96685958, G_loss: 1.31351531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pseulki/anaconda3/envs/myenv/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 2] [ 100/ 468] D_loss: 1.01915467, G_loss: 1.27695477\n",
      "Epoch: [ 2] [ 200/ 468] D_loss: 0.97965437, G_loss: 1.36600912\n",
      "Epoch: [ 2] [ 300/ 468] D_loss: 1.02389574, G_loss: 1.11102450\n",
      "Epoch: [ 2] [ 400/ 468] D_loss: 1.03224218, G_loss: 1.08895791\n",
      "Epoch: [ 3] [ 100/ 468] D_loss: 0.97423315, G_loss: 1.37309456\n",
      "Epoch: [ 3] [ 200/ 468] D_loss: 1.02497053, G_loss: 1.46939242\n",
      "Epoch: [ 3] [ 300/ 468] D_loss: 0.92513692, G_loss: 1.42075467\n",
      "Epoch: [ 3] [ 400/ 468] D_loss: 1.04585361, G_loss: 1.32140732\n",
      "Epoch: [ 4] [ 100/ 468] D_loss: 1.05246997, G_loss: 1.45350337\n",
      "Epoch: [ 4] [ 200/ 468] D_loss: 1.12713385, G_loss: 1.05427825\n",
      "Epoch: [ 4] [ 300/ 468] D_loss: 1.11999846, G_loss: 1.89888525\n",
      "Epoch: [ 4] [ 400/ 468] D_loss: 1.05544233, G_loss: 1.35864329\n",
      "Epoch: [ 5] [ 100/ 468] D_loss: 1.10269225, G_loss: 1.55846763\n",
      "Epoch: [ 5] [ 200/ 468] D_loss: 0.97971928, G_loss: 1.63332987\n",
      "Epoch: [ 5] [ 300/ 468] D_loss: 0.91346729, G_loss: 1.61674762\n",
      "Epoch: [ 5] [ 400/ 468] D_loss: 0.92214721, G_loss: 1.35158753\n",
      "Epoch: [ 6] [ 100/ 468] D_loss: 0.92738783, G_loss: 1.21821976\n",
      "Epoch: [ 6] [ 200/ 468] D_loss: 0.95370167, G_loss: 1.47957158\n",
      "Epoch: [ 6] [ 300/ 468] D_loss: 0.90208685, G_loss: 1.50237918\n",
      "Epoch: [ 6] [ 400/ 468] D_loss: 0.76608485, G_loss: 1.83479583\n",
      "Epoch: [ 7] [ 100/ 468] D_loss: 0.91045398, G_loss: 1.42920780\n",
      "Epoch: [ 7] [ 200/ 468] D_loss: 0.86165059, G_loss: 1.61985457\n",
      "Epoch: [ 7] [ 300/ 468] D_loss: 0.83533746, G_loss: 1.92624736\n",
      "Epoch: [ 7] [ 400/ 468] D_loss: 0.87270373, G_loss: 1.62143195\n",
      "Epoch: [ 8] [ 100/ 468] D_loss: 0.88745958, G_loss: 2.12056422\n",
      "Epoch: [ 8] [ 200/ 468] D_loss: 0.85997295, G_loss: 1.83683050\n",
      "Epoch: [ 8] [ 300/ 468] D_loss: 0.92858326, G_loss: 2.06421685\n",
      "Epoch: [ 8] [ 400/ 468] D_loss: 0.97709483, G_loss: 1.28624022\n",
      "Epoch: [ 9] [ 100/ 468] D_loss: 0.96729219, G_loss: 1.61597419\n",
      "Epoch: [ 9] [ 200/ 468] D_loss: 0.96307755, G_loss: 2.22880101\n",
      "Epoch: [ 9] [ 300/ 468] D_loss: 0.75436950, G_loss: 1.93463707\n",
      "Epoch: [ 9] [ 400/ 468] D_loss: 1.01234365, G_loss: 1.74337149\n",
      "Epoch: [10] [ 100/ 468] D_loss: 0.69330680, G_loss: 1.98785257\n",
      "Epoch: [10] [ 200/ 468] D_loss: 0.81034428, G_loss: 2.30529308\n",
      "Epoch: [10] [ 300/ 468] D_loss: 0.81224275, G_loss: 2.02343893\n",
      "Epoch: [10] [ 400/ 468] D_loss: 0.85299540, G_loss: 1.80739427\n",
      "Epoch: [11] [ 100/ 468] D_loss: 0.72405785, G_loss: 2.06910944\n",
      "Epoch: [11] [ 200/ 468] D_loss: 1.01998699, G_loss: 2.09934783\n",
      "Epoch: [11] [ 300/ 468] D_loss: 0.75989729, G_loss: 2.22574496\n",
      "Epoch: [11] [ 400/ 468] D_loss: 0.74985600, G_loss: 1.75208235\n",
      "Epoch: [12] [ 100/ 468] D_loss: 0.84735668, G_loss: 1.78408515\n",
      "Epoch: [12] [ 200/ 468] D_loss: 0.78243840, G_loss: 1.72869253\n",
      "Epoch: [12] [ 300/ 468] D_loss: 0.84446084, G_loss: 1.99181378\n",
      "Epoch: [12] [ 400/ 468] D_loss: 0.76327133, G_loss: 1.93899310\n",
      "Epoch: [13] [ 100/ 468] D_loss: 0.82425606, G_loss: 1.97070348\n",
      "Epoch: [13] [ 200/ 468] D_loss: 0.84030396, G_loss: 1.77402091\n",
      "Epoch: [13] [ 300/ 468] D_loss: 0.87504923, G_loss: 2.09304047\n",
      "Epoch: [13] [ 400/ 468] D_loss: 0.78843653, G_loss: 1.91990149\n",
      "Epoch: [14] [ 100/ 468] D_loss: 0.78445989, G_loss: 1.79149735\n",
      "Epoch: [14] [ 200/ 468] D_loss: 0.78154922, G_loss: 2.12099028\n",
      "Epoch: [14] [ 300/ 468] D_loss: 0.99751085, G_loss: 1.62374246\n",
      "Epoch: [14] [ 400/ 468] D_loss: 0.70306861, G_loss: 1.71558809\n",
      "Epoch: [15] [ 100/ 468] D_loss: 0.78787208, G_loss: 2.01381969\n",
      "Epoch: [15] [ 200/ 468] D_loss: 0.76455158, G_loss: 1.62512231\n",
      "Epoch: [15] [ 300/ 468] D_loss: 0.88105941, G_loss: 1.86798310\n",
      "Epoch: [15] [ 400/ 468] D_loss: 0.83418298, G_loss: 2.13422799\n",
      "Epoch: [16] [ 100/ 468] D_loss: 0.79002857, G_loss: 2.04217434\n",
      "Epoch: [16] [ 200/ 468] D_loss: 0.92984682, G_loss: 1.97189748\n",
      "Epoch: [16] [ 300/ 468] D_loss: 0.95054102, G_loss: 1.45559621\n",
      "Epoch: [16] [ 400/ 468] D_loss: 0.78060722, G_loss: 1.61879337\n",
      "Epoch: [17] [ 100/ 468] D_loss: 1.00889182, G_loss: 1.38622320\n",
      "Epoch: [17] [ 200/ 468] D_loss: 0.74135947, G_loss: 1.82765877\n",
      "Epoch: [17] [ 300/ 468] D_loss: 1.00518727, G_loss: 1.95754218\n",
      "Epoch: [17] [ 400/ 468] D_loss: 0.95241642, G_loss: 1.66356039\n",
      "Epoch: [18] [ 100/ 468] D_loss: 0.87026668, G_loss: 1.35733414\n",
      "Epoch: [18] [ 200/ 468] D_loss: 0.81844699, G_loss: 2.06305599\n",
      "Epoch: [18] [ 300/ 468] D_loss: 0.94202006, G_loss: 1.78029370\n",
      "Epoch: [18] [ 400/ 468] D_loss: 0.86017561, G_loss: 1.79616141\n",
      "Epoch: [19] [ 100/ 468] D_loss: 0.81807023, G_loss: 1.83062601\n",
      "Epoch: [19] [ 200/ 468] D_loss: 0.99867177, G_loss: 1.76241052\n",
      "Epoch: [19] [ 300/ 468] D_loss: 0.85568404, G_loss: 1.65314543\n",
      "Epoch: [19] [ 400/ 468] D_loss: 1.01505351, G_loss: 1.39490926\n",
      "Epoch: [20] [ 100/ 468] D_loss: 0.89626795, G_loss: 1.65126681\n",
      "Epoch: [20] [ 200/ 468] D_loss: 0.88993448, G_loss: 1.48757315\n",
      "Epoch: [20] [ 300/ 468] D_loss: 0.90839124, G_loss: 1.61480987\n",
      "Epoch: [20] [ 400/ 468] D_loss: 0.74944127, G_loss: 1.99254453\n",
      "Epoch: [21] [ 100/ 468] D_loss: 0.83892190, G_loss: 1.90343904\n",
      "Epoch: [21] [ 200/ 468] D_loss: 0.84986627, G_loss: 1.92119586\n",
      "Epoch: [21] [ 300/ 468] D_loss: 0.72738981, G_loss: 2.13009214\n",
      "Epoch: [21] [ 400/ 468] D_loss: 1.15506482, G_loss: 1.73194396\n",
      "Epoch: [22] [ 100/ 468] D_loss: 0.89640659, G_loss: 1.94437206\n",
      "Epoch: [22] [ 200/ 468] D_loss: 1.02906299, G_loss: 1.49087906\n",
      "Epoch: [22] [ 300/ 468] D_loss: 0.91141945, G_loss: 1.43414271\n",
      "Epoch: [22] [ 400/ 468] D_loss: 0.86030781, G_loss: 1.51931036\n",
      "Epoch: [23] [ 100/ 468] D_loss: 0.83365583, G_loss: 1.70001829\n",
      "Epoch: [23] [ 200/ 468] D_loss: 1.07235014, G_loss: 1.81458807\n",
      "Epoch: [23] [ 300/ 468] D_loss: 0.79467303, G_loss: 2.09821272\n",
      "Epoch: [23] [ 400/ 468] D_loss: 0.81091726, G_loss: 2.01616478\n",
      "Epoch: [24] [ 100/ 468] D_loss: 0.97165811, G_loss: 1.94833386\n",
      "Epoch: [24] [ 200/ 468] D_loss: 1.00968719, G_loss: 1.17040253\n",
      "Epoch: [24] [ 300/ 468] D_loss: 0.77755266, G_loss: 1.74734294\n",
      "Epoch: [24] [ 400/ 468] D_loss: 0.92345393, G_loss: 1.43441474\n",
      "Epoch: [25] [ 100/ 468] D_loss: 1.05946076, G_loss: 1.73050189\n",
      "Epoch: [25] [ 200/ 468] D_loss: 0.98157805, G_loss: 2.13807297\n",
      "Epoch: [25] [ 300/ 468] D_loss: 0.98632419, G_loss: 1.13560176\n",
      "Epoch: [25] [ 400/ 468] D_loss: 0.75656426, G_loss: 1.81544423\n",
      "Avg one epoch time: 4.80, total 25 epochs time: 120.21\n",
      "Training finish!... save training results\n",
      " [*] Training finished!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\"\"\"train GAN\"\"\"\n",
    "\n",
    "gan.train()\n",
    "print(\" [*] Training finished!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [*] Testing finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pseulki/anaconda3/envs/myenv/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "\"\"\"test GAN\"\"\"\n",
    "\n",
    "# visualize learned generator\n",
    "gan.visualize_results(epoch)\n",
    "print(\" [*] Testing finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./results/GAN/GAN_FC_generate_animation.gif\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<img src=\"./results/GAN/GAN_FC_generate_animation.gif\">')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
